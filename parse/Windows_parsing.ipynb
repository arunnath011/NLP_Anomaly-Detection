{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45acdf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from logparser import Spell, Drain\n",
    "from tqdm import tqdm\n",
    "#from logdeep.dataset.session import sliding_window\n",
    "import glob\n",
    "tqdm.pandas()\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "\n",
    "# In the first column of the log, \"-\" indicates non-alert messages while others are alert messages.\n",
    "def count_anomaly(log_path):\n",
    "    total_size = 0\n",
    "    normal_size = 0\n",
    "    with open(log_path, errors='ignore') as f:\n",
    "        for line in f:\n",
    "            total_size += 1\n",
    "            if line.split('')[0] == '-':\n",
    "                normal_size += 1\n",
    "    print(\"total size {}, abnormal size {}\".format(total_size, total_size - normal_size))\n",
    "\n",
    "\n",
    "def deeplog_file_generator(filename, df, features):\n",
    "    with open(filename, 'w') as f:\n",
    "        for _, row in df.iterrows():\n",
    "            for val in zip(*row[features]):\n",
    "                f.write(','.join([str(v) for v in val]) + ' ')\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sample_raw_data(data_file, output_file, sample_window_size, sample_step_size):\n",
    "    # sample 1M by sliding window, abnormal rate is over 2%\n",
    "    sample_data = []\n",
    "    labels = []\n",
    "    idx = 0\n",
    "\n",
    "    # spirit dataset can start from the 2Mth line, as there are many abnormal lines gathering in the first 2M\n",
    "    with open(data_file, 'r', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            labels.append(line.split()[0] != '-')\n",
    "            sample_data.append(line)\n",
    "\n",
    "            if len(labels) == sample_window_size:\n",
    "                abnormal_rate = sum(np.array(labels)) / len(labels)\n",
    "                print(f\"{idx + 1} lines, abnormal rate {abnormal_rate}\")\n",
    "                break\n",
    "\n",
    "            idx += 1\n",
    "            if idx % sample_step_size == 0:\n",
    "                print(f\"Process {round(idx/sample_window_size * 100,4)} % raw data\", end='\\r')\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.writelines(sample_data)\n",
    "\n",
    "    print(\"Sampling done\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a1d252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_log(input_dir, output_dir, log_file, parser_type):\n",
    "    #log_format = '<Label> <Id> <Date> <Admin> <Month> <Day> <Time> <AdminAddr> <Content>'\n",
    "    log_format = '<Date> <Time>, <Type>             <Admin>  <Content>'\n",
    "    regex = [\n",
    "        r'(0x)[0-9a-fA-F]+',  # hexadecimal\n",
    "        r'\\d+\\.\\d+\\.\\d+\\.\\d+',\n",
    "        r'(?<=Warning: we failed to resolve data source name )[\\w\\s]+',\n",
    "        r'\\d+'\n",
    "    ]\n",
    "    keep_para = False\n",
    "    if parser_type == \"drain\":\n",
    "        # the hyper parameter is set according to http://jmzhu.logpai.com/pub/pjhe_icws2017.pdf\n",
    "        st = 0.3  # Similarity threshold\n",
    "        depth = 3  # Depth of all leaf nodes\n",
    "\n",
    "        # Drain is modified\n",
    "        parser = Drain.LogParser(log_format,\n",
    "                                 indir=input_dir,\n",
    "                                 outdir=output_dir,\n",
    "                                 depth=depth,\n",
    "                                 st=st,\n",
    "                                 rex=regex,\n",
    "                                 keep_para=keep_para, maxChild=1000)\n",
    "        parser.parse(log_file)\n",
    "\n",
    "    elif parser_type == \"spell\":\n",
    "        tau = 0.35\n",
    "        parser = Spell.LogParser(indir=data_dir,\n",
    "                                 outdir=output_dir,\n",
    "                                 log_format=log_format,\n",
    "                                 tau=tau,\n",
    "                                 rex=regex,\n",
    "                                 keep_para=keep_para)\n",
    "        parser.parse(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19150d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wrd00.txt', 'wrd01.txt', 'wrd02.txt', 'wrd03.txt', 'wrd04.txt', 'wrd05.txt', 'wrd06.txt', 'wrd07.txt']\n",
      "\n",
      "Parsing wrd00.txt\n",
      "Parsing file: ../dataset/Windows\\wrd00.txt\n",
      "Total size after encoding is 9995 10000\n",
      "Parsing done. [Time taken: 0:00:00.618187]\n",
      "\n",
      "Parsing wrd01.txt\n",
      "Parsing file: ../dataset/Windows\\wrd01.txt\n",
      "Total size after encoding is 9999 10000\n",
      "Parsing done. [Time taken: 0:00:00.588087]\n",
      "\n",
      "Parsing wrd02.txt\n",
      "Parsing file: ../dataset/Windows\\wrd02.txt\n",
      "Total size after encoding is 9972 10000\n",
      "Parsing done. [Time taken: 0:00:00.700026]\n",
      "\n",
      "Parsing wrd03.txt\n",
      "Parsing file: ../dataset/Windows\\wrd03.txt\n",
      "Total size after encoding is 10000 10000\n",
      "Parsing done. [Time taken: 0:00:00.719724]\n",
      "\n",
      "Parsing wrd04.txt\n",
      "Parsing file: ../dataset/Windows\\wrd04.txt\n",
      "Total size after encoding is 10000 10000\n",
      "Parsing done. [Time taken: 0:00:00.694393]\n",
      "\n",
      "Parsing wrd05.txt\n",
      "Parsing file: ../dataset/Windows\\wrd05.txt\n",
      "Total size after encoding is 10000 10000\n",
      "Parsing done. [Time taken: 0:00:00.728774]\n",
      "\n",
      "Parsing wrd06.txt\n",
      "Parsing file: ../dataset/Windows\\wrd06.txt\n",
      "Total size after encoding is 10000 10000\n",
      "Parsing done. [Time taken: 0:00:00.742855]\n",
      "\n",
      "Parsing wrd07.txt\n",
      "Parsing file: ../dataset/Windows\\wrd07.txt\n",
      "Total size after encoding is 10000 10000\n",
      "Parsing done. [Time taken: 0:00:00.713459]\n",
      "\n",
      "Transforming wrd00.txt\n",
      "\n",
      "Transforming wrd01.txt\n",
      "\n",
      "Transforming wrd02.txt\n",
      "\n",
      "Transforming wrd03.txt\n",
      "\n",
      "Transforming wrd04.txt\n",
      "\n",
      "Transforming wrd05.txt\n",
      "\n",
      "Transforming wrd06.txt\n",
      "\n",
      "Transforming wrd07.txt\n",
      "done!!\n"
     ]
    }
   ],
   "source": [
    "##File parser\n",
    "\n",
    "data_dir = os.path.expanduser(\"../dataset/Windows\")\n",
    "output_dir = \"../output/windows/\"\n",
    "raw_log_file = \"xaa.txt\"\n",
    "sample_log_file = \"xaa.txt\"\n",
    "sample_window_size = 2*10**7\n",
    "sample_step_size = 10**4\n",
    "window_name = ''\n",
    "log_file = sample_log_file\n",
    "\n",
    "parser_type = 'drain'\n",
    "#mins\n",
    "window_size = 1\n",
    "step_size = 0.5\n",
    "train_ratio = 6000\n",
    "\n",
    "########\n",
    "# count anomaly\n",
    "########\n",
    "# count_anomaly(data_dir + log_file)\n",
    "# sys.exit()\n",
    "\n",
    "import glob\n",
    "file_list = glob.glob(data_dir +\"/*.txt\")\n",
    "#########\n",
    "# sample raw data\n",
    "#########\n",
    "#sample_raw_data(data_dir+raw_log_file, data_dir+sample_log_file, sample_window_size, sample_step_size )\n",
    "\n",
    "a = glob.glob(data_dir +\"/*.*\") \n",
    "file_list=[os.path.basename(list_item) for list_item in a]\n",
    "print(file_list)\n",
    "##########\n",
    "# Parser #\n",
    "#########\n",
    "for file in file_list:\n",
    "    print(\"\\nParsing\", file)\n",
    "    parse_log(data_dir, output_dir, file, parser_type)\n",
    "\n",
    "\n",
    "##################\n",
    "# Transformation #\n",
    "##################\n",
    "for file in file_list:\n",
    "    print(\"\\nTransforming\", file)\n",
    "    df = pd.read_csv(f'{output_dir}{file}_structured.csv')\n",
    "    df['Label'] = 0\n",
    "\n",
    "    df.loc[df['Content'].str.contains(\"HRESULT\"), \"Label\"] = 1\n",
    "    df.to_excel(f'{output_dir}{file}_labeled.xlsx')  \n",
    "    \n",
    "\n",
    " \n",
    "# csv files in the path\n",
    "file_list = glob.glob(output_dir + \"*_labeled.xlsx\")\n",
    " \n",
    "# list of excel files we want to merge.\n",
    "# pd.read_excel(file_path) reads the excel\n",
    "# data into pandas dataframe.\n",
    "excl_list = []\n",
    " \n",
    "for file in file_list:\n",
    "    excl_list.append(pd.read_excel(file))\n",
    " \n",
    "# create a new dataframe to store the\n",
    "# merged excel file.\n",
    "excl_merged = pd.DataFrame()\n",
    " \n",
    "for excl_file in excl_list:\n",
    "     \n",
    "    # appends the data into the excl_merged\n",
    "    # dataframe.\n",
    "    excl_merged = excl_merged.append(\n",
    "      excl_file, ignore_index=True)\n",
    " \n",
    "# exports the dataframe into excel file with\n",
    "# specified name.\n",
    "excl_merged.to_excel('total_output.xlsx', index=False)\n",
    "print(\"done!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff10880",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deeplog_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17104/4007956687.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Train #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#########\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdf_normal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeeplog_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdeeplog_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Label\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mdf_normal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_normal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#shuffle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mnormal_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_normal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'deeplog_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# sampling with sliding window\n",
    "#deeplog_df = sliding_window(df[[\"timestamp\", \"Label\", \"EventId\", \"deltaT\" <Date> <Time>, <Type>             <Admin>  <Content>]],\n",
    "                            #para={\"window_size\": float(window_size)*60, \"step_size\": float(step_size) * 60}\n",
    "                            #)\n",
    "#output_dir += window_name\n",
    "\n",
    "#########\n",
    "# Train #\n",
    "#########\n",
    "df_normal = deeplog_df[deeplog_df[\"Label\"] == 0]\n",
    "df_normal = df_normal.sample(frac=1, random_state=12).reset_index(drop=True) #shuffle\n",
    "normal_len = len(df_normal)\n",
    "train_len = int(train_ratio) if train_ratio >= 1 else int(normal_len * train_ratio)\n",
    "\n",
    "train = df_normal[:train_len]\n",
    "deeplog_file_generator(os.path.join(output_dir,'train'), train, [\"EventId\"])\n",
    "print(\"training size {}\".format(train_len))\n",
    "\n",
    "\n",
    "###############\n",
    "# Test Normal #\n",
    "###############\n",
    "test_normal = df_normal[train_len:]\n",
    "deeplog_file_generator(os.path.join(output_dir, 'test_normal'), test_normal, [\"EventId\"])\n",
    "print(\"test normal size {}\".format(normal_len - train_len))\n",
    "\n",
    "\n",
    "#################\n",
    "# Test Abnormal #\n",
    "#################\n",
    "df_abnormal = deeplog_df[deeplog_df[\"Label\"] == 1]\n",
    "deeplog_file_generator(os.path.join(output_dir,'test_abnormal'), df_abnormal, [\"EventId\"])\n",
    "print('test abnormal size {}'.format(len(df_abnormal)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045d2c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the path to csv files\n",
    "path = \"C:/downloads\"\n",
    " \n",
    "# csv files in the path\n",
    "file_list = glob.glob(path + \"/*.xlsx\")\n",
    " \n",
    "# list of excel files we want to merge.\n",
    "# pd.read_excel(file_path) reads the excel\n",
    "# data into pandas dataframe.\n",
    "excl_list = []\n",
    " \n",
    "for file in file_list:\n",
    "    excl_list.append(pd.read_excel(file))\n",
    " \n",
    "# create a new dataframe to store the\n",
    "# merged excel file.\n",
    "excl_merged = pd.DataFrame()\n",
    " \n",
    "for excl_file in excl_list:\n",
    "     \n",
    "    # appends the data into the excl_merged\n",
    "    # dataframe.\n",
    "    excl_merged = excl_merged.append(\n",
    "      excl_file, ignore_index=True)\n",
    " \n",
    "# exports the dataframe into excel file with\n",
    "# specified name.\n",
    "excl_merged.to_excel('total_food_sales.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
